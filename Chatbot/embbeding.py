import fitz  # PyMuPDF
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import pickle
import os

def extract_text_from_pdf(pdf_path):
    """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if not os.path.exists(pdf_path):
        print(f"âŒ PDF íŒŒì¼ '{pdf_path}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
        
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text()
    doc.close()
    return text

def split_text(text, max_length=500):
    """í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ìˆëŠ” ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    sentences = text.split('\n')
    chunks = []
    chunk = ""
    for sentence in sentences:
        if len(chunk) + len(sentence) < max_length:
            chunk += sentence + " "
        else:
            if chunk.strip():
                chunks.append(chunk.strip())
            chunk = sentence + " "
    if chunk.strip():
        chunks.append(chunk.strip())
    return chunks

def build_faiss_index(pdf_path):
    """PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    print("ğŸ“„ PDF ë¬¸ì„œë¥¼ ë²¡í„°í™”í•˜ëŠ” ì¤‘...")
    
    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
    text = extract_text_from_pdf(pdf_path)
    if text is None:
        return
    
    # í…ìŠ¤íŠ¸ ë¶„í• 
    chunks = split_text(text)
    print(f"âœ… {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")
    
    # ë²¡í„° ì„ë² ë”© ìƒì„±
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(chunks)
    
    # FAISS ì¸ë±ìŠ¤ ìƒì„±
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(np.array(embeddings))
    
    # íŒŒì¼ ì €ì¥
    faiss.write_index(index, "faiss_index.bin")
    with open("chunks.pkl", "wb") as f:
        pickle.dump(chunks, f)
    
    print("âœ… ë¬¸ì„œ ë²¡í„°í™” ë° ì €ì¥ ì™„ë£Œ!")

if __name__ == "__main__":
    build_faiss_index("document.pdf")
